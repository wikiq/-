> 本文由 [简悦 SimpRead](http://ksria.com/simpread/) 转码， 原文地址 [github.com](https://github.com/ggerganov/llama.cpp/pull/1998)

> I put together a simple web-chat that demonstrates how to use the SSE(ish) streaming in the server ex......

I put together a simple web-chat that demonstrates how to use the SSE(ish) streaming in the server example. I also went ahead and served it from the root url, to make the server a bit more approachable.

I tried to match the spirit of llama.cpp and used minimalistic js dependencies and went with the ozempic css style of ggml.ai.

Initially I went for no-js dependencies but gave up and used a few minimal that i'm importing from js cdns instead of adding them here. Let me know if you agree with this approach. I needed microsoft's fetch-event-source for using event-source over POST (super disappointed that browsers don't support that, actually) and preact+htm for keeping my sanity with all this state,. The upshot is that everything is in one small html file. Speaking of- there is probably a better (and less fragile) way to include the server.html in the cpp binary, but it's been 25 years since I worked with cpp tooling.

(updated screenshot)  
[![](https://user-images.githubusercontent.com/347/250649993-bda45f50-12d0-45b1-8ca5-d232bc968ee7.png)](https://user-images.githubusercontent.com/347/250649993-bda45f50-12d0-45b1-8ca5-d232bc968ee7.png)